{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e2f437-0a9b-431d-9335-15753a2ebffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 25 04:15:24 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off | 00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   38C    P2             118W / 420W |   1915MiB / 24576MiB |     19%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 2060 ...    Off | 00000000:03:00.0  On |                  N/A |\n",
      "| 29%   48C    P0              42W / 184W |    135MiB /  8192MiB |      1%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1466      G   /usr/lib/xorg/Xorg                          298MiB |\n",
      "|    0   N/A  N/A      1704      G   /usr/bin/gnome-shell                         29MiB |\n",
      "|    0   N/A  N/A      3836      G   ...seed-version=20240424-080043.603000       57MiB |\n",
      "|    0   N/A  N/A     42817      C   ...x/miniconda3/envs/rapids/bin/python     1434MiB |\n",
      "|    0   N/A  N/A     43301      G   EXCEL.EXE                                    54MiB |\n",
      "|    1   N/A  N/A      1466      G   /usr/lib/xorg/Xorg                          133MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb3e8d4-50e0-4e5f-8d3b-7a8d080b960d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 2060 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# # Check if CUDA is available\n",
    "# cuda_available = torch.cuda.is_available()\n",
    "# print(f\"CUDA available: {cuda_available}\")\n",
    "\n",
    "# # Set the device to GPU if CUDA is available\n",
    "# device = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" #select 3090 Super\n",
    "\n",
    "if torch.cuda.device_count() >= 2:\n",
    "    print(f\"{torch.cuda.device_count()} GPUs are available\")\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "for i in range(num_gpus):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "device = torch.device(\"cuda:0\")  # Default GPU\n",
    "    \n",
    "# # Example of moving a tensor to GPU\n",
    "# if cuda_available:\n",
    "#     #Get ID\n",
    "#     gpu_id = torch.cuda.current_device()\n",
    "#     print(f\"GPU ID being used: {gpu_id}\")\n",
    "\n",
    "#     # Get the name of the current GPU\n",
    "#     gpu_name = torch.cuda.get_device_name(gpu_id)\n",
    "#     print(f\"GPU Name: {gpu_name}\")\n",
    "\n",
    "# else:\n",
    "#     print(\"CUDA is not available. No GPU is being used.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc203648-a5a7-47f5-98a1-703076e640b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ebbad6d-0a56-4beb-9f6d-e5431b097c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory():\n",
    "    allocated = torch.cuda.memory_allocated()\n",
    "    max_allocated = torch.cuda.max_memory_allocated()\n",
    "    print(f\"Current memory allocated: {allocated / 1024**3:.2f} GB\")\n",
    "    print(f\"Max memory allocated during this session: {max_allocated / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb607f91-8f77-4bc1-b71e-10a5945dccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"caching_allocator\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e488f8-3082-461e-99a5-c24a7f01fe3e",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aed3664-26ca-49eb-bce5-4be244c9adb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9edfa74-bdfb-4bde-a0cd-98d3163a7e78",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42ea653f-1939-4fe4-80f5-a57a4e337b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "#Read smote data back to nb ad cuda df\n",
    "X_train = pd.read_parquet(\"../capstone_data/archive/SMOTE_train_test_split/X_train_smote.parquet\")\n",
    "y_train = pd.read_parquet(\"../capstone_data/archive/SMOTE_train_test_split/y_train_smote.parquet\")['label']\n",
    "X_test = pd.read_parquet(\"../capstone_data/archive/SMOTE_train_test_split/X_test_smote.parquet\")\n",
    "y_test = pd.read_parquet(\"../capstone_data/archive/SMOTE_train_test_split/y_test_smote.parquet\")['label']\n",
    "\n",
    "print(type(X_train))\n",
    "print(type(y_train))\n",
    "print(type(X_test))\n",
    "print(type(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17106828-0665-4dc8-887a-fa1583614e37",
   "metadata": {},
   "source": [
    "# Standardize numerical Cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d1c1f22-daff-44f6-b295-d6a38a273fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "numeric_columns = ['duration', #extremely right skewed\n",
    "                   'orig_bytes', #extremely right skewed\n",
    "                   'resp_bytes', #extremely right skewed\n",
    "                   'orig_pkts', #extremely right skewed\n",
    "                   'resp_pkts', #extremely right skewed\n",
    "                   'orig_ip_bytes', ##extremely right skewed\n",
    "                   'resp_ip_bytes'] #extremely right skewed\n",
    "\n",
    "#Log transformation for extremely right skewed distributions\n",
    "constant = 1e-6\n",
    "X_train[numeric_columns] = X_train[numeric_columns].apply(lambda x: np.log(x + constant))\n",
    "X_test[numeric_columns] = X_test[numeric_columns].apply(lambda x: np.log(x + constant))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c9463f-a634-4635-a359-562c33ddb055",
   "metadata": {},
   "source": [
    "# Convert to NumPy Array then create tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2804bd84-596e-4394-a54a-bad13ac05de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #convert to numpy array\n",
    "# X_train_np = X_train.to_numpy()\n",
    "# X_train_np = X_train_np.reshape(X_train_np.shape[0], 1, X_train_np.shape[1])\n",
    "# y_train_np = y_train.to_numpy()\n",
    "# X_test_np = X_test.to_numpy()\n",
    "# y_test_np = y_test.to_numpy()\n",
    "\n",
    "# # Convert to tensors suitable for PyTorch\n",
    "# X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)  # Reshape for BCEWithLogitsLoss\n",
    "\n",
    "# X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "# y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)  # Reshape for BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "478fd925-fd2d-4306-ae6b-b178bb46af85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Create TensorDatasets\n",
    "# train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "# test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# # Create DataLoaders\n",
    "# batch_size = 32\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55115716-483a-44eb-bbca-4314a6c53a0c",
   "metadata": {},
   "source": [
    "# Set RNN:LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "485e9107-a8b5-4102-84a4-986a0fc70871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max memory allocated during this session: 9.69 GB\n",
    "# 106.60491490364075\n",
    "# Epoch [10/10], Loss: 0.6622\n",
    "# Accuracy of the model on the test data: 67.19%\n",
    "\n",
    "\n",
    "# # Convert to tensors suitable for PyTorch\n",
    "# X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
    "# y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to(device)  # Reshape for BCEWithLogitsLoss\n",
    "# X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "# y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1).to(device)  # Reshape for BCEWithLogitsLoss\n",
    "\n",
    "# # Create Tensor datasets\n",
    "# train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "# test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# # Create DataLoaders\n",
    "# batch_size = 1228800  # You can adjust this based on your GPU memory\n",
    "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# class LSTMClassifier(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "#         super(LSTMClassifier, self).__init__()\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.layer_dim = layer_dim\n",
    "#         self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Check if the input tensor is 2D and if so, unsqueeze it to 3D\n",
    "#         if x.dim() == 2:\n",
    "#             x = x.unsqueeze(1)  # Add a sequence dimension\n",
    "\n",
    "#         h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
    "#         c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
    "#         out, _ = self.lstm(x, (h0, c0))\n",
    "#         out = self.fc(out[:, -1, :])  # Assuming that you want the last time step\n",
    "#         return out\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = LSTMClassifier(input_dim=X_train.shape[1], hidden_dim=50, layer_dim=1, output_dim=1)\n",
    "# model = model.to(device)\n",
    "\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Training loop\n",
    "# num_epochs = 10\n",
    "# for epoch in range(num_epochs):\n",
    "#     start_time = time.time()\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for inputs, labels in train_loader:\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     print_gpu_memory()\n",
    "#     print(time.time() - start_time)\n",
    "    \n",
    "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}')\n",
    "\n",
    "# # Evaluate the model\n",
    "# model.eval()\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# with torch.no_grad():\n",
    "#     for inputs, labels in test_loader:\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "#         outputs = model(inputs)\n",
    "#         predicted = torch.sigmoid(outputs) > 0.5\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "# accuracy = 100 * correct / total\n",
    "# print(f'Accuracy of the model on the test data: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b07bd-f135-48c1-b433-9d9ce0fcd062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7de90a-5ede-48fc-80d9-888c3da1aab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8afd154-a424-4ace-810d-6da89f067b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646e4b10-7a86-41a7-bff7-51dc44c13bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f182670-1b7a-4e91-9542-59f0703638b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bef2ae-04d5-436c-9a3c-10dda815989d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3da83a-ef86-4125-ab9f-565b047f8a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7dcf08-1e55-4dc4-867c-f3060668cdaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e164f263-0846-4234-ab1c-a00485c18965",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 205791: Last Loss 0.6673\n",
      "Epoch 1: Average Training Loss 0.6888\n",
      "New best accuracy 0.5872, Epoch 1: \n",
      "Epoch 1 Time: 385.14 seconds\n",
      "Epoch 2, Batch 205791: Last Loss 0.6742\n",
      "Epoch 2: Average Training Loss 0.6662\n",
      "New best accuracy 0.6584, Epoch 2: \n",
      "Epoch 2 Time: 391.49 seconds\n",
      "Epoch 3, Batch 205791: Last Loss 0.6015\n",
      "Epoch 3: Average Training Loss 0.6490\n",
      "New best accuracy 0.6728, Epoch 3: \n",
      "Epoch 3 Time: 387.28 seconds\n",
      "Epoch 4, Batch 205791: Last Loss 0.4763\n",
      "Epoch 4: Average Training Loss 0.6392\n",
      "Epoch 4 Time: 389.54 seconds\n",
      "Epoch 5, Batch 205791: Last Loss 0.5946\n",
      "Epoch 5: Average Training Loss 0.6320\n",
      "Epoch 5 Time: 388.45 seconds\n",
      "Epoch 6, Batch 205791: Last Loss 0.6211\n",
      "Epoch 6: Average Training Loss 0.6295\n",
      "Epoch 6 Time: 389.40 seconds\n",
      "Epoch 7, Batch 205791: Last Loss 0.5654\n",
      "Epoch 7: Average Training Loss 0.6273\n",
      "New best accuracy 0.6790, Epoch 7: \n",
      "Epoch 7 Time: 389.09 seconds\n",
      "Epoch 8, Batch 205791: Last Loss 0.7956\n",
      "Epoch 8: Average Training Loss 0.6266\n",
      "Epoch 8 Time: 388.06 seconds\n",
      "Epoch 9, Batch 205791: Last Loss 0.6450\n",
      "Epoch 9: Average Training Loss 0.6251\n",
      "New best accuracy 0.6819, Epoch 9: \n",
      "Epoch 9 Time: 389.75 seconds\n",
      "Epoch 10, Batch 205791: Last Loss 0.5206\n",
      "Epoch 10: Average Training Loss 0.6241\n",
      "New best accuracy 0.6855, Epoch 10: \n",
      "Epoch 10 Time: 387.33 seconds\n",
      "Epoch 11, Batch 205791: Last Loss 0.6602\n",
      "Epoch 11: Average Training Loss 0.6226\n",
      "New best accuracy 0.6856, Epoch 11: \n",
      "Epoch 11 Time: 391.55 seconds\n",
      "Epoch 12, Batch 205791: Last Loss 0.7725\n",
      "Epoch 12: Average Training Loss 0.6214\n",
      "Epoch 12 Time: 384.15 seconds\n",
      "Epoch 13, Batch 205791: Last Loss 0.4853\n",
      "Epoch 13: Average Training Loss 0.6203\n",
      "Epoch 13 Time: 390.28 seconds\n",
      "Epoch 14, Batch 205791: Last Loss 0.5689\n",
      "Epoch 14: Average Training Loss 0.6170\n",
      "New best accuracy 0.6978, Epoch 14: \n",
      "Epoch 14 Time: 386.50 seconds\n",
      "Epoch 15, Batch 205791: Last Loss 0.7245\n",
      "Epoch 15: Average Training Loss 0.6057\n",
      "New best accuracy 0.7068, Epoch 15: \n",
      "Epoch 15 Time: 391.19 seconds\n",
      "Epoch 16, Batch 205791: Last Loss 0.5135\n",
      "Epoch 16: Average Training Loss 0.5950\n",
      "New best accuracy 0.7181, Epoch 16: \n",
      "Epoch 16 Time: 384.28 seconds\n",
      "Epoch 17, Batch 205791: Last Loss 0.6327\n",
      "Epoch 17: Average Training Loss 0.5856\n",
      "Epoch 17 Time: 390.11 seconds\n",
      "Epoch 18, Batch 205791: Last Loss 0.3670\n",
      "Epoch 18: Average Training Loss 0.5836\n",
      "New best accuracy 0.7247, Epoch 18: \n",
      "Epoch 18 Time: 385.91 seconds\n",
      "Epoch 19, Batch 205791: Last Loss 0.5488\n",
      "Epoch 19: Average Training Loss 0.5831\n",
      "Epoch 19 Time: 389.80 seconds\n",
      "Epoch 20, Batch 205791: Last Loss 0.6680\n",
      "Epoch 20: Average Training Loss 0.5823\n",
      "Epoch 20 Time: 391.73 seconds\n",
      "Epoch 21, Batch 205791: Last Loss 0.6635\n",
      "Epoch 21: Average Training Loss 0.5816\n",
      "Epoch 21 Time: 389.96 seconds\n",
      "Epoch 22, Batch 205791: Last Loss 0.7459\n",
      "Epoch 22: Average Training Loss 0.5811\n",
      "Epoch 22 Time: 389.39 seconds\n",
      "Epoch 23, Batch 205791: Last Loss 0.5323\n",
      "Epoch 23: Average Training Loss 0.5809\n",
      "Epoch 23 Time: 388.32 seconds\n",
      "Epoch 24, Batch 205791: Last Loss 0.4821\n",
      "Epoch 24: Average Training Loss 0.5799\n",
      "Epoch 24 Time: 390.00 seconds\n",
      "Epoch 25, Batch 205791: Last Loss 0.4169\n",
      "Epoch 25: Average Training Loss 0.5794\n",
      "Epoch 25 Time: 386.87 seconds\n",
      "Epoch 26, Batch 205791: Last Loss 0.4144\n",
      "Epoch 26: Average Training Loss 0.5785\n",
      "Epoch 26 Time: 392.02 seconds\n",
      "Epoch 27, Batch 205791: Last Loss 0.4777\n",
      "Epoch 27: Average Training Loss 0.5772\n",
      "Epoch 27 Time: 385.83 seconds\n",
      "Epoch 28, Batch 205791: Last Loss 0.5778\n",
      "Epoch 28: Average Training Loss 0.5761\n",
      "Epoch 28 Time: 390.32 seconds\n",
      "Epoch 29, Batch 205791: Last Loss 0.4198\n",
      "Epoch 29: Average Training Loss 0.5755\n",
      "New best accuracy 0.7261, Epoch 29: \n",
      "Epoch 29 Time: 385.51 seconds\n",
      "Epoch 30, Batch 205791: Last Loss 0.5863\n",
      "Epoch 30: Average Training Loss 0.5751\n",
      "Epoch 30 Time: 390.62 seconds\n",
      "Epoch 31, Batch 205791: Last Loss 0.4509\n",
      "Epoch 31: Average Training Loss 0.5747\n",
      "Epoch 31 Time: 384.85 seconds\n",
      "Epoch 32, Batch 205791: Last Loss 0.5770\n",
      "Epoch 32: Average Training Loss 0.5741\n",
      "Epoch 32 Time: 391.24 seconds\n",
      "Epoch 33, Batch 205791: Last Loss 0.4995\n",
      "Epoch 33: Average Training Loss 0.5739\n",
      "Epoch 33 Time: 387.04 seconds\n",
      "Epoch 34, Batch 205791: Last Loss 0.5805\n",
      "Epoch 34: Average Training Loss 0.5737\n",
      "Epoch 34 Time: 390.72 seconds\n",
      "Epoch 35, Batch 205791: Last Loss 0.6949\n",
      "Epoch 35: Average Training Loss 0.5737\n",
      "Epoch 35 Time: 388.48 seconds\n",
      "Epoch 36, Batch 205791: Last Loss 0.6666\n",
      "Epoch 36: Average Training Loss 0.5735\n",
      "Epoch 36 Time: 390.19 seconds\n",
      "Epoch 37, Batch 205791: Last Loss 0.5054\n",
      "Epoch 37: Average Training Loss 0.5733\n",
      "New best accuracy 0.7289, Epoch 37: \n",
      "Epoch 37 Time: 388.83 seconds\n",
      "Epoch 38, Batch 205791: Last Loss 0.4450\n",
      "Epoch 38: Average Training Loss 0.5727\n",
      "Epoch 38 Time: 388.62 seconds\n",
      "Epoch 39, Batch 205791: Last Loss 0.4137\n",
      "Epoch 39: Average Training Loss 0.5726\n",
      "Epoch 39 Time: 389.93 seconds\n",
      "Epoch 40, Batch 205791: Last Loss 0.6045\n",
      "Epoch 40: Average Training Loss 0.5721\n",
      "Epoch 40 Time: 386.54 seconds\n",
      "Epoch 41, Batch 205791: Last Loss 0.4864\n",
      "Epoch 41: Average Training Loss 0.5720\n",
      "Epoch 41 Time: 390.24 seconds\n",
      "Epoch 42, Batch 205791: Last Loss 0.4778\n",
      "Epoch 42: Average Training Loss 0.5718\n",
      "Epoch 42 Time: 386.00 seconds\n",
      "Epoch 43, Batch 205791: Last Loss 0.4032\n",
      "Epoch 43: Average Training Loss 0.5717\n",
      "Epoch 43 Time: 390.80 seconds\n",
      "Epoch 44, Batch 205791: Last Loss 0.6622\n",
      "Epoch 44: Average Training Loss 0.5717\n",
      "Epoch 44 Time: 386.79 seconds\n",
      "Epoch 45, Batch 205791: Last Loss 0.7154\n",
      "Epoch 45: Average Training Loss 0.5715\n",
      "Epoch 45 Time: 391.20 seconds\n",
      "Epoch 46, Batch 205791: Last Loss 0.4242\n",
      "Epoch 46: Average Training Loss 0.5712\n",
      "Epoch 46 Time: 385.83 seconds\n",
      "Epoch 47, Batch 205791: Last Loss 0.3444\n",
      "Epoch 47: Average Training Loss 0.5709\n",
      "Epoch 47 Time: 391.64 seconds\n",
      "Epoch 48, Batch 205791: Last Loss 0.3669\n",
      "Epoch 48: Average Training Loss 0.5712\n",
      "Epoch 48 Time: 386.11 seconds\n",
      "Epoch 49, Batch 205791: Last Loss 0.8001\n",
      "Epoch 49: Average Training Loss 0.5705\n",
      "Epoch 49 Time: 390.90 seconds\n",
      "Epoch 50, Batch 205791: Last Loss 0.6252\n",
      "Epoch 50: Average Training Loss 0.5707\n",
      "Epoch 50 Time: 387.49 seconds\n",
      "Best Model with hidden_size=64, num_layers=2, learning_rate=1e-06, accuracy=0.7289\n",
      "Epoch 1, Batch 205791: Last Loss 0.6494\n",
      "Epoch 1: Average Training Loss 0.6890\n",
      "New best accuracy 0.6485, Epoch 1: \n",
      "Epoch 1 Time: 413.86 seconds\n",
      "Epoch 2, Batch 205791: Last Loss 0.7052\n",
      "Epoch 2: Average Training Loss 0.6694\n",
      "New best accuracy 0.6551, Epoch 2: \n",
      "Epoch 2 Time: 412.02 seconds\n",
      "Epoch 3, Batch 205791: Last Loss 0.5965\n",
      "Epoch 3: Average Training Loss 0.6602\n",
      "New best accuracy 0.6802, Epoch 3: \n",
      "Epoch 3 Time: 414.22 seconds\n",
      "Epoch 4, Batch 205791: Last Loss 0.6525\n",
      "Epoch 4: Average Training Loss 0.6532\n",
      "New best accuracy 0.6979, Epoch 4: \n",
      "Epoch 4 Time: 411.27 seconds\n",
      "Epoch 5, Batch 205791: Last Loss 0.6689\n",
      "Epoch 5: Average Training Loss 0.6491\n",
      "New best accuracy 0.7000, Epoch 5: \n",
      "Epoch 5 Time: 414.23 seconds\n",
      "Epoch 6, Batch 205791: Last Loss 0.6646\n",
      "Epoch 6: Average Training Loss 0.6473\n",
      "New best accuracy 0.7001, Epoch 6: \n",
      "Epoch 6 Time: 412.00 seconds\n",
      "Epoch 7, Batch 205791: Last Loss 0.5778\n",
      "Epoch 7: Average Training Loss 0.6440\n",
      "New best accuracy 0.7102, Epoch 7: \n",
      "Epoch 7 Time: 413.24 seconds\n",
      "Epoch 8, Batch 205791: Last Loss 0.7136\n",
      "Epoch 8: Average Training Loss 0.6402\n",
      "New best accuracy 0.7106, Epoch 8: \n",
      "Epoch 8 Time: 412.80 seconds\n",
      "Epoch 9, Batch 205791: Last Loss 0.5575\n",
      "Epoch 9: Average Training Loss 0.6377\n",
      "Epoch 9 Time: 414.40 seconds\n",
      "Epoch 10, Batch 205791: Last Loss 0.6348\n",
      "Epoch 10: Average Training Loss 0.6367\n",
      "New best accuracy 0.7111, Epoch 10: \n",
      "Epoch 10 Time: 412.43 seconds\n",
      "Epoch 11, Batch 205791: Last Loss 0.6216\n",
      "Epoch 11: Average Training Loss 0.6355\n",
      "New best accuracy 0.7181, Epoch 11: \n",
      "Epoch 11 Time: 414.53 seconds\n",
      "Epoch 12, Batch 205791: Last Loss 0.4907\n",
      "Epoch 12: Average Training Loss 0.6331\n",
      "New best accuracy 0.7195, Epoch 12: \n",
      "Epoch 12 Time: 412.55 seconds\n",
      "Epoch 13, Batch 205791: Last Loss 0.6039\n",
      "Epoch 13: Average Training Loss 0.6311\n",
      "Epoch 13 Time: 413.42 seconds\n",
      "Epoch 14, Batch 205791: Last Loss 0.6361\n",
      "Epoch 14: Average Training Loss 0.6294\n",
      "Epoch 14 Time: 413.60 seconds\n",
      "Epoch 15, Batch 205791: Last Loss 0.6533\n",
      "Epoch 15: Average Training Loss 0.6236\n",
      "Epoch 15 Time: 413.76 seconds\n",
      "Epoch 16, Batch 205791: Last Loss 0.4710\n",
      "Epoch 16: Average Training Loss 0.6124\n",
      "Epoch 16 Time: 412.92 seconds\n",
      "Epoch 17, Batch 205791: Last Loss 0.4510\n",
      "Epoch 17: Average Training Loss 0.6102\n",
      "Epoch 17 Time: 413.75 seconds\n",
      "Epoch 18, Batch 205791: Last Loss 0.6844\n",
      "Epoch 18: Average Training Loss 0.6072\n",
      "Epoch 18 Time: 412.55 seconds\n",
      "Epoch 19, Batch 205791: Last Loss 0.5730\n",
      "Epoch 19: Average Training Loss 0.6027\n",
      "Epoch 19 Time: 413.77 seconds\n",
      "Epoch 20, Batch 205791: Last Loss 0.6043\n",
      "Epoch 20: Average Training Loss 0.6014\n",
      "Epoch 20 Time: 412.99 seconds\n",
      "Epoch 21, Batch 205791: Last Loss 0.5889\n",
      "Epoch 21: Average Training Loss 0.6005\n",
      "Epoch 21 Time: 413.79 seconds\n",
      "Epoch 22, Batch 205791: Last Loss 0.4635\n",
      "Epoch 22: Average Training Loss 0.6000\n",
      "Epoch 22 Time: 413.40 seconds\n",
      "Epoch 23, Batch 205791: Last Loss 0.6478\n",
      "Epoch 23: Average Training Loss 0.5993\n",
      "Epoch 23 Time: 413.09 seconds\n",
      "Epoch 24, Batch 205791: Last Loss 0.5678\n",
      "Epoch 24: Average Training Loss 0.5985\n",
      "Epoch 24 Time: 412.83 seconds\n",
      "Epoch 25, Batch 205791: Last Loss 0.4789\n",
      "Epoch 25: Average Training Loss 0.5980\n",
      "Epoch 25 Time: 412.63 seconds\n",
      "Epoch 26, Batch 205791: Last Loss 0.4770\n",
      "Epoch 26: Average Training Loss 0.5974\n",
      "Epoch 26 Time: 413.58 seconds\n",
      "Epoch 27, Batch 205791: Last Loss 0.6121\n",
      "Epoch 27: Average Training Loss 0.5966\n",
      "Epoch 27 Time: 412.56 seconds\n",
      "Epoch 28, Batch 205791: Last Loss 0.5921\n",
      "Epoch 28: Average Training Loss 0.5960\n",
      "Epoch 28 Time: 413.01 seconds\n",
      "Epoch 29, Batch 205791: Last Loss 0.3816\n",
      "Epoch 29: Average Training Loss 0.5953\n",
      "Epoch 29 Time: 413.11 seconds\n",
      "Epoch 30, Batch 205791: Last Loss 0.7019\n",
      "Epoch 30: Average Training Loss 0.5944\n",
      "Epoch 30 Time: 415.42 seconds\n",
      "Epoch 31, Batch 205791: Last Loss 0.5399\n",
      "Epoch 31: Average Training Loss 0.5939\n",
      "Epoch 31 Time: 413.82 seconds\n",
      "Epoch 32, Batch 205791: Last Loss 0.6436\n",
      "Epoch 32: Average Training Loss 0.5933\n",
      "Epoch 32 Time: 413.55 seconds\n",
      "Epoch 33, Batch 205791: Last Loss 0.5612\n",
      "Epoch 33: Average Training Loss 0.5921\n",
      "Epoch 33 Time: 414.29 seconds\n",
      "Epoch 34, Batch 205791: Last Loss 0.7367\n",
      "Epoch 34: Average Training Loss 0.5912\n",
      "Epoch 34 Time: 413.50 seconds\n",
      "Epoch 35, Batch 205791: Last Loss 0.5552\n",
      "Epoch 35: Average Training Loss 0.5904\n",
      "Epoch 35 Time: 412.79 seconds\n",
      "Epoch 36, Batch 205791: Last Loss 0.6160\n",
      "Epoch 36: Average Training Loss 0.5888\n",
      "Epoch 36 Time: 413.11 seconds\n",
      "Epoch 37, Batch 205791: Last Loss 0.6006\n",
      "Epoch 37: Average Training Loss 0.5858\n",
      "Epoch 37 Time: 413.19 seconds\n",
      "Epoch 38, Batch 205791: Last Loss 0.7217\n",
      "Epoch 38: Average Training Loss 0.5836\n",
      "Epoch 38 Time: 413.40 seconds\n",
      "Epoch 39, Batch 205791: Last Loss 0.7950\n",
      "Epoch 39: Average Training Loss 0.5794\n",
      "Epoch 39 Time: 412.41 seconds\n",
      "Epoch 40, Batch 205791: Last Loss 0.5940\n",
      "Epoch 40: Average Training Loss 0.5770\n",
      "Epoch 40 Time: 413.32 seconds\n",
      "Epoch 41, Batch 205791: Last Loss 0.7126\n",
      "Epoch 41: Average Training Loss 0.5750\n",
      "Epoch 41 Time: 412.26 seconds\n",
      "Epoch 42, Batch 205791: Last Loss 0.6200\n",
      "Epoch 42: Average Training Loss 0.5729\n",
      "Epoch 42 Time: 413.56 seconds\n",
      "Epoch 43, Batch 205791: Last Loss 0.4728\n",
      "Epoch 43: Average Training Loss 0.5702\n",
      "New best accuracy 0.7201, Epoch 43: \n",
      "Epoch 43 Time: 413.90 seconds\n",
      "Epoch 44, Batch 205791: Last Loss 0.3761\n",
      "Epoch 44: Average Training Loss 0.5688\n",
      "Epoch 44 Time: 414.61 seconds\n",
      "Epoch 45, Batch 205791: Last Loss 0.7483\n",
      "Epoch 45: Average Training Loss 0.5679\n",
      "Epoch 45 Time: 411.87 seconds\n",
      "Epoch 46, Batch 205791: Last Loss 0.5495\n",
      "Epoch 46: Average Training Loss 0.5660\n",
      "Epoch 46 Time: 413.95 seconds\n",
      "Epoch 47, Batch 205791: Last Loss 0.4697\n",
      "Epoch 47: Average Training Loss 0.5647\n",
      "New best accuracy 0.7229, Epoch 47: \n",
      "Epoch 47 Time: 411.64 seconds\n",
      "Epoch 48, Batch 205791: Last Loss 0.4400\n",
      "Epoch 48: Average Training Loss 0.5639\n",
      "Epoch 48 Time: 413.79 seconds\n",
      "Epoch 49, Batch 205791: Last Loss 0.5171\n",
      "Epoch 49: Average Training Loss 0.5593\n",
      "Epoch 49 Time: 410.77 seconds\n",
      "Epoch 50, Batch 205791: Last Loss 0.5447\n",
      "Epoch 50: Average Training Loss 0.5564\n",
      "New best accuracy 0.7230, Epoch 50: \n",
      "Epoch 50 Time: 414.81 seconds\n",
      "Best Model with hidden_size=64, num_layers=3, learning_rate=1e-06, accuracy=0.7230\n",
      "Epoch 1, Batch 205791: Last Loss 0.6301\n",
      "Epoch 1: Average Training Loss 0.6749\n",
      "New best accuracy 0.6606, Epoch 1: \n",
      "Epoch 1 Time: 436.49 seconds\n",
      "Epoch 2, Batch 205791: Last Loss 0.7691\n",
      "Epoch 2: Average Training Loss 0.6434\n",
      "New best accuracy 0.7086, Epoch 2: \n",
      "Epoch 2 Time: 437.73 seconds\n",
      "Epoch 3, Batch 205791: Last Loss 0.7434\n",
      "Epoch 3: Average Training Loss 0.6093\n",
      "Epoch 3 Time: 436.80 seconds\n",
      "Epoch 4, Batch 205791: Last Loss 0.5640\n",
      "Epoch 4: Average Training Loss 0.5954\n",
      "New best accuracy 0.7168, Epoch 4: \n",
      "Epoch 4 Time: 437.35 seconds\n",
      "Epoch 5, Batch 205791: Last Loss 0.4777\n",
      "Epoch 5: Average Training Loss 0.5886\n",
      "New best accuracy 0.7243, Epoch 5: \n",
      "Epoch 5 Time: 438.60 seconds\n",
      "Epoch 6, Batch 205791: Last Loss 0.6940\n",
      "Epoch 6: Average Training Loss 0.5855\n",
      "Epoch 6 Time: 435.14 seconds\n",
      "Epoch 7, Batch 205791: Last Loss 0.6004\n",
      "Epoch 7: Average Training Loss 0.5745\n",
      "Epoch 7 Time: 440.33 seconds\n",
      "Epoch 8, Batch 205791: Last Loss 0.4732\n",
      "Epoch 8: Average Training Loss 0.5642\n",
      "New best accuracy 0.7257, Epoch 8: \n",
      "Epoch 8 Time: 436.53 seconds\n",
      "Epoch 9, Batch 205791: Last Loss 0.6161\n",
      "Epoch 9: Average Training Loss 0.5584\n",
      "Epoch 9 Time: 441.71 seconds\n",
      "Epoch 10, Batch 205791: Last Loss 0.6543\n",
      "Epoch 10: Average Training Loss 0.5685\n",
      "Epoch 10 Time: 435.17 seconds\n",
      "Epoch 11, Batch 205791: Last Loss 0.3918\n",
      "Epoch 11: Average Training Loss 0.5664\n",
      "Epoch 11 Time: 441.09 seconds\n",
      "Epoch 12, Batch 205791: Last Loss 0.5992\n",
      "Epoch 12: Average Training Loss 0.5653\n",
      "New best accuracy 0.7263, Epoch 12: \n",
      "Epoch 12 Time: 434.64 seconds\n",
      "Epoch 13, Batch 205791: Last Loss 0.5296\n",
      "Epoch 13: Average Training Loss 0.5969\n",
      "Epoch 13 Time: 440.70 seconds\n",
      "Epoch 14, Batch 205791: Last Loss 0.4520\n",
      "Epoch 14: Average Training Loss 0.5760\n",
      "Epoch 14 Time: 435.52 seconds\n",
      "Epoch 15, Batch 205791: Last Loss 0.4495\n",
      "Epoch 15: Average Training Loss 0.5741\n",
      "Epoch 15 Time: 440.56 seconds\n",
      "Epoch 16, Batch 205791: Last Loss 0.4925\n",
      "Epoch 16: Average Training Loss 0.5728\n",
      "Epoch 16 Time: 435.69 seconds\n",
      "Epoch 17, Batch 205791: Last Loss 0.6164\n",
      "Epoch 17: Average Training Loss 0.5716\n",
      "Epoch 17 Time: 440.32 seconds\n",
      "Epoch 18, Batch 205791: Last Loss 0.8805\n",
      "Epoch 18: Average Training Loss 0.5707\n",
      "Epoch 18 Time: 436.97 seconds\n",
      "Epoch 19, Batch 205791: Last Loss 0.4869\n",
      "Epoch 19: Average Training Loss 0.5707\n",
      "Epoch 19 Time: 438.67 seconds\n",
      "Epoch 20, Batch 205791: Last Loss 0.8389\n",
      "Epoch 20: Average Training Loss 0.5706\n",
      "Epoch 20 Time: 438.31 seconds\n",
      "Epoch 21, Batch 205791: Last Loss 0.4469\n",
      "Epoch 21: Average Training Loss 0.5705\n",
      "Epoch 21 Time: 455.43 seconds\n",
      "Epoch 22, Batch 205791: Last Loss 0.4649\n",
      "Epoch 22: Average Training Loss 0.5705\n",
      "Epoch 22 Time: 442.51 seconds\n",
      "Epoch 23, Batch 205791: Last Loss 0.5358\n",
      "Epoch 23: Average Training Loss 0.5704\n",
      "Epoch 23 Time: 473.06 seconds\n",
      "Epoch 24, Batch 205791: Last Loss 0.5151\n",
      "Epoch 24: Average Training Loss 0.5705\n",
      "Epoch 24 Time: 490.26 seconds\n",
      "Epoch 25, Batch 205791: Last Loss 0.6369\n",
      "Epoch 25: Average Training Loss 0.5704\n",
      "Epoch 25 Time: 483.17 seconds\n",
      "Epoch 26, Batch 205791: Last Loss 0.6494\n",
      "Epoch 26: Average Training Loss 0.5702\n",
      "Epoch 26 Time: 466.07 seconds\n",
      "Epoch 27, Batch 205791: Last Loss 0.6189\n",
      "Epoch 27: Average Training Loss 0.5703\n",
      "Epoch 27 Time: 464.97 seconds\n",
      "Epoch 28, Batch 205791: Last Loss 0.5868\n",
      "Epoch 28: Average Training Loss 0.5703\n",
      "Epoch 28 Time: 443.04 seconds\n",
      "Epoch 29, Batch 205791: Last Loss 0.7008\n",
      "Epoch 29: Average Training Loss 0.5702\n",
      "Epoch 29 Time: 435.25 seconds\n",
      "Epoch 30, Batch 205791: Last Loss 0.5565\n",
      "Epoch 30: Average Training Loss 0.5701\n",
      "Epoch 30 Time: 440.32 seconds\n",
      "Epoch 31, Batch 205791: Last Loss 0.7526\n",
      "Epoch 31: Average Training Loss 0.5701\n",
      "Epoch 31 Time: 435.80 seconds\n",
      "Epoch 32, Batch 205791: Last Loss 0.5680\n",
      "Epoch 32: Average Training Loss 0.5700\n",
      "Epoch 32 Time: 439.64 seconds\n",
      "Epoch 33, Batch 205791: Last Loss 0.7104\n",
      "Epoch 33: Average Training Loss 0.5700\n",
      "Epoch 33 Time: 436.17 seconds\n",
      "Epoch 34, Batch 205791: Last Loss 0.7427\n",
      "Epoch 34: Average Training Loss 0.5698\n",
      "Epoch 34 Time: 438.57 seconds\n",
      "Epoch 35, Batch 205791: Last Loss 0.5616\n",
      "Epoch 35: Average Training Loss 0.5700\n",
      "Epoch 35 Time: 436.76 seconds\n",
      "Epoch 36, Batch 205791: Last Loss 0.7108\n",
      "Epoch 36: Average Training Loss 0.5698\n",
      "Epoch 36 Time: 438.81 seconds\n",
      "Epoch 37, Batch 205791: Last Loss 0.7035\n",
      "Epoch 37: Average Training Loss 0.5698\n",
      "Epoch 37 Time: 438.08 seconds\n",
      "Epoch 38, Batch 205791: Last Loss 0.4344\n",
      "Epoch 38: Average Training Loss 0.5698\n",
      "Epoch 38 Time: 436.41 seconds\n",
      "Epoch 39, Batch 205791: Last Loss 0.6042\n",
      "Epoch 39: Average Training Loss 0.5697\n",
      "Epoch 39 Time: 438.29 seconds\n",
      "Epoch 40, Batch 205791: Last Loss 0.5927\n",
      "Epoch 40: Average Training Loss 0.5698\n",
      "Epoch 40 Time: 436.04 seconds\n",
      "Epoch 41, Batch 205791: Last Loss 0.5741\n",
      "Epoch 41: Average Training Loss 0.5696\n",
      "Epoch 41 Time: 440.45 seconds\n",
      "Epoch 42, Batch 205791: Last Loss 0.5377\n",
      "Epoch 42: Average Training Loss 0.5695\n",
      "Epoch 42 Time: 435.27 seconds\n",
      "Epoch 43, Batch 205791: Last Loss 0.6818\n",
      "Epoch 43: Average Training Loss 0.5695\n",
      "Epoch 43 Time: 440.40 seconds\n",
      "Epoch 44, Batch 205791: Last Loss 0.5375\n",
      "Epoch 44: Average Training Loss 0.5694\n",
      "Epoch 44 Time: 435.57 seconds\n",
      "Epoch 45, Batch 205791: Last Loss 0.5655\n",
      "Epoch 45: Average Training Loss 0.5693\n",
      "Epoch 45 Time: 439.54 seconds\n",
      "Epoch 46, Batch 205791: Last Loss 0.4950\n",
      "Epoch 46: Average Training Loss 0.5693\n",
      "Epoch 46 Time: 435.34 seconds\n",
      "Epoch 47, Batch 205791: Last Loss 0.5542\n",
      "Epoch 47: Average Training Loss 0.5693\n",
      "Epoch 47 Time: 440.57 seconds\n",
      "Epoch 48, Batch 205791: Last Loss 0.5643\n",
      "Epoch 48: Average Training Loss 0.5693\n",
      "Epoch 48 Time: 435.40 seconds\n",
      "Epoch 49, Batch 205791: Last Loss 0.4917\n",
      "Epoch 49: Average Training Loss 0.5692\n",
      "Epoch 49 Time: 440.72 seconds\n",
      "Epoch 50, Batch 205791: Last Loss 0.6182\n",
      "Epoch 50: Average Training Loss 0.5692\n",
      "Epoch 50 Time: 434.95 seconds\n",
      "Best Model with hidden_size=128, num_layers=2, learning_rate=1e-06, accuracy=0.7263\n",
      "Epoch 1, Batch 205791: Last Loss 0.5840\n",
      "Epoch 1: Average Training Loss 0.6718\n",
      "New best accuracy 0.6841, Epoch 1: \n",
      "Epoch 1 Time: 505.46 seconds\n",
      "Epoch 2, Batch 205791: Last Loss 0.5778\n",
      "Epoch 2: Average Training Loss 0.6470\n",
      "New best accuracy 0.6858, Epoch 2: \n",
      "Epoch 2 Time: 506.10 seconds\n",
      "Epoch 3, Batch 205791: Last Loss 0.5510\n",
      "Epoch 3: Average Training Loss 0.6432\n",
      "New best accuracy 0.6956, Epoch 3: \n",
      "Epoch 3 Time: 525.05 seconds\n",
      "Epoch 4, Batch 205791: Last Loss 0.7004\n",
      "Epoch 4: Average Training Loss 0.6399\n",
      "Epoch 4 Time: 510.00 seconds\n",
      "Epoch 5, Batch 205791: Last Loss 0.5907\n",
      "Epoch 5: Average Training Loss 0.6365\n",
      "New best accuracy 0.7039, Epoch 5: \n",
      "Epoch 5 Time: 504.56 seconds\n",
      "Epoch 6, Batch 205791: Last Loss 0.4969\n",
      "Epoch 6: Average Training Loss 0.6335\n",
      "New best accuracy 0.7116, Epoch 6: \n",
      "Epoch 6 Time: 506.30 seconds\n",
      "Epoch 7, Batch 205791: Last Loss 0.5799\n",
      "Epoch 7: Average Training Loss 0.6233\n",
      "New best accuracy 0.7162, Epoch 7: \n",
      "Epoch 7 Time: 507.42 seconds\n",
      "Epoch 8, Batch 205791: Last Loss 0.7534\n",
      "Epoch 8: Average Training Loss 0.6004\n",
      "Epoch 8 Time: 505.15 seconds\n",
      "Epoch 9, Batch 205791: Last Loss 0.6691\n",
      "Epoch 9: Average Training Loss 0.5911\n",
      "New best accuracy 0.7185, Epoch 9: \n",
      "Epoch 9 Time: 514.39 seconds\n",
      "Epoch 10, Batch 205791: Last Loss 0.6245\n",
      "Epoch 10: Average Training Loss 0.5855\n",
      "New best accuracy 0.7242, Epoch 10: \n",
      "Epoch 10 Time: 491.54 seconds\n",
      "Epoch 11, Batch 205791: Last Loss 0.6372\n",
      "Epoch 11: Average Training Loss 0.5822\n",
      "Epoch 11 Time: 474.54 seconds\n",
      "Epoch 12, Batch 205791: Last Loss 0.4992\n",
      "Epoch 12: Average Training Loss 0.5799\n",
      "Epoch 12 Time: 484.49 seconds\n",
      "Epoch 13, Batch 205791: Last Loss 0.6690\n",
      "Epoch 13: Average Training Loss 0.5788\n",
      "New best accuracy 0.7249, Epoch 13: \n",
      "Epoch 13 Time: 496.80 seconds\n",
      "Epoch 14, Batch 205791: Last Loss 0.6075\n",
      "Epoch 14: Average Training Loss 0.5784\n",
      "Epoch 14 Time: 516.33 seconds\n",
      "Epoch 15, Batch 205791: Last Loss 0.4253\n",
      "Epoch 15: Average Training Loss 0.5781\n",
      "New best accuracy 0.7262, Epoch 15: \n",
      "Epoch 15 Time: 512.05 seconds\n",
      "Epoch 16, Batch 205791: Last Loss 0.5614\n",
      "Epoch 16: Average Training Loss 0.5776\n",
      "Epoch 16 Time: 489.00 seconds\n",
      "Epoch 17, Batch 205791: Last Loss 0.6621\n",
      "Epoch 17: Average Training Loss 0.5774\n",
      "Epoch 17 Time: 485.86 seconds\n",
      "Epoch 18, Batch 205791: Last Loss 0.5048\n",
      "Epoch 18: Average Training Loss 0.5774\n",
      "Epoch 18 Time: 474.46 seconds\n",
      "Epoch 19, Batch 205791: Last Loss 0.7952\n",
      "Epoch 19: Average Training Loss 0.5772\n",
      "Epoch 19 Time: 474.53 seconds\n",
      "Epoch 20, Batch 205791: Last Loss 0.5464\n",
      "Epoch 20: Average Training Loss 0.5774\n",
      "Epoch 20 Time: 474.93 seconds\n",
      "Epoch 21, Batch 205791: Last Loss 0.6732\n",
      "Epoch 21: Average Training Loss 0.5769\n",
      "New best accuracy 0.7263, Epoch 21: \n",
      "Epoch 21 Time: 475.40 seconds\n",
      "Epoch 22, Batch 205791: Last Loss 0.6323\n",
      "Epoch 22: Average Training Loss 0.5768\n",
      "Epoch 22 Time: 475.27 seconds\n",
      "Epoch 23, Batch 205791: Last Loss 0.4212\n",
      "Epoch 23: Average Training Loss 0.5751\n",
      "Epoch 23 Time: 474.58 seconds\n",
      "Epoch 24, Batch 205791: Last Loss 0.5149\n",
      "Epoch 24: Average Training Loss 0.5638\n",
      "Epoch 24 Time: 493.88 seconds\n",
      "Epoch 25, Batch 205791: Last Loss 0.6186\n",
      "Epoch 25: Average Training Loss 0.5603\n",
      "Epoch 25 Time: 511.24 seconds\n",
      "Epoch 26, Batch 205791: Last Loss 0.4482\n",
      "Epoch 26: Average Training Loss 0.5556\n",
      "Epoch 26 Time: 510.96 seconds\n",
      "Epoch 27, Batch 205791: Last Loss 0.4926\n",
      "Epoch 27: Average Training Loss 0.5543\n",
      "Epoch 27 Time: 571.52 seconds\n",
      "Epoch 28, Batch 205791: Last Loss 0.3353\n",
      "Epoch 28: Average Training Loss 0.5533\n",
      "Epoch 28 Time: 484.89 seconds\n",
      "Epoch 29, Batch 205791: Last Loss 0.3679\n",
      "Epoch 29: Average Training Loss 0.5524\n",
      "Epoch 29 Time: 484.76 seconds\n",
      "Epoch 30, Batch 205791: Last Loss 0.5557\n",
      "Epoch 30: Average Training Loss 0.5521\n",
      "Epoch 30 Time: 536.03 seconds\n",
      "Epoch 31, Batch 205791: Last Loss 0.4920\n",
      "Epoch 31: Average Training Loss 0.5513\n",
      "Epoch 31 Time: 532.65 seconds\n",
      "Epoch 32, Batch 205791: Last Loss 0.6407\n",
      "Epoch 32: Average Training Loss 0.5499\n",
      "Epoch 32 Time: 474.74 seconds\n",
      "Epoch 33, Batch 205791: Last Loss 0.5132\n",
      "Epoch 33: Average Training Loss 0.5485\n",
      "Epoch 33 Time: 474.20 seconds\n",
      "Epoch 34, Batch 205791: Last Loss 0.4075\n",
      "Epoch 34: Average Training Loss 0.5477\n",
      "Epoch 34 Time: 474.37 seconds\n",
      "Epoch 35, Batch 205791: Last Loss 0.4383\n",
      "Epoch 35: Average Training Loss 0.5470\n",
      "Epoch 35 Time: 474.65 seconds\n",
      "Epoch 36, Batch 205791: Last Loss 0.4763\n",
      "Epoch 36: Average Training Loss 0.5467\n",
      "Epoch 36 Time: 474.77 seconds\n",
      "Epoch 37, Batch 205791: Last Loss 0.4973\n",
      "Epoch 37: Average Training Loss 0.5503\n",
      "Epoch 37 Time: 474.34 seconds\n",
      "Epoch 38, Batch 205791: Last Loss 0.6895\n",
      "Epoch 38: Average Training Loss 0.5461\n",
      "Epoch 38 Time: 496.56 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "def df_to_tensor(df):\n",
    "    return torch.tensor(df.values, dtype=torch.float32).to(device)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "# Create Tensor datasets\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64 #524288 for 3090rtx\n",
    "train_loader = DataLoader(train_data, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True\n",
    "                          )\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Define the LSTM Model\n",
    "class BinaryLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(BinaryLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.5 if num_layers > 1 else 0)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, (hn, _) = self.lstm(x)\n",
    "        # Handle cases where the LSTM output directly corresponds to hidden states without additional sequence dimension\n",
    "        last_time_step = output[:, -1, :] if output.dim() == 3 else hn.squeeze(0)\n",
    "        out = self.linear(last_time_step)\n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "# Train and evaluate function\n",
    "def train_and_evaluate(model, train_loader, test_loader, optimizer, criterion, num_epochs):\n",
    "    best_accuracy = 0\n",
    "    best_model = None\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_batches = 0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            total_batches += 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #print(f'Epoch {epoch+1}, Iteration {i+1}, Loss: {loss.item()}')\n",
    "\n",
    "            # Print loss for each epoch iteration\n",
    "        print(f'Epoch {epoch+1}, Batch {total_batches}: Last Loss {loss.item():.4f}')\n",
    "\n",
    "        average_loss = total_loss / total_batches\n",
    "        print(f'Epoch {epoch+1}: Average Training Loss {average_loss:.4f}')\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                loss = criterion(outputs, labels)  # Calculate loss\n",
    "                total_loss += loss.item()\n",
    "            val_loss = total_loss / len(test_loader)\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        accuracy = correct / total\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = model\n",
    "            print(f'New best accuracy {best_accuracy:.4f}, Epoch {epoch+1}: ')\n",
    "\n",
    "        \n",
    "\n",
    "        print(f'Epoch {epoch+1} Time: {time.time() - start_time:.2f} seconds')\n",
    "\n",
    "    return best_model, best_accuracy\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X_train.shape[1] #number of features\n",
    "hidden_sizes = [64,128,256]\n",
    "num_layers_list = [2,3]\n",
    "learning_rates = [0.000001]\n",
    "num_epochs = 50\n",
    "\n",
    "# Grid search for best hyperparameters\n",
    "for hidden_size in hidden_sizes:\n",
    "    for num_layers in num_layers_list:\n",
    "        for learning_rate in learning_rates:            \n",
    "            model = BinaryLSTM(input_size, hidden_size, num_layers)\n",
    "            model.cuda()\n",
    "\n",
    "\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            criterion = nn.BCELoss()\n",
    "            #LR Scheduleer\n",
    "            scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "            total_loss = []\n",
    "            best_model, best_accuracy = train_and_evaluate(model, train_loader, test_loader, optimizer, criterion, num_epochs)            \n",
    "            print(f'Best Model with hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, accuracy={best_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061b8bef-d894-4ec0-9c0e-cd217dd35c82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "626272f0-7431-49f3-ac1b-03791bbc51ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model.state_dict(), 'best_lstm_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b300cf5-1021-4ba2-94af-dae6e475b0fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0749f616-4bb1-4fd7-b786-c51aca89b96a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
